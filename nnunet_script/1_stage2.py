# -*- coding: utf-8 -*-
"""
Stage 2 æ•°æ®ç”Ÿæˆå™¨ - å¤šä¸ªåˆ†ç¦»çš„ç¥ç»å…ƒ
æ”¹è¿›ç‰ˆæœ¬ï¼š
- æ•°æ®åˆ’åˆ†ï¼šå‰ 500 ä¸ªå•ç¥ç»å…ƒï¼ˆ400 è®­ç»ƒ + 100 éªŒè¯ï¼‰ï¼Œå…¶ä½™ç•™ä½œæµ‹è¯•
- æ•°æ®å¢å¼ºï¼šéšæœºç¿»è½¬ã€90Â° æ—‹è½¬
- å…ƒæ•°æ®è®°å½•ï¼šä¿å­˜å®Œæ•´é…ç½®ä¿¡æ¯
- å¯è§†åŒ–æ£€æŸ¥ï¼šç”Ÿæˆæœ€å¤§æŠ•å½±å›¾
"""
import os
import json
import random
import traceback
from datetime import datetime
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass, asdict

import numpy as np
import zarr
import numcodecs
from scipy.ndimage import zoom, center_of_mass


# =========================
# é…ç½®
# =========================
@dataclass
class Stage2Config:
    """Stage 2 é…ç½®å‚æ•°"""
    # è·¯å¾„
    input_dir: str = "/data/wangfeiran/code/brainbow/datasets/fisbe/single_neurons"
    output_dir: str = "/data/wangfeiran/code/brainbow/datasets/fisbe/curriculum_data/stage2"
    
    # ç›®æ ‡ä½“ç§¯å¤§å°
    target_size: Tuple[int, int, int] = (200, 250, 250)
    
    # æ•°æ®åˆ’åˆ†ï¼ˆåªä½¿ç”¨å‰ 500 ä¸ªï¼‰
    total_neurons_to_use: int = 500     # åªä½¿ç”¨å‰ 500 ä¸ª
    train_pool_size: int = 400          # å‰ 400 ä¸ªç”¨äºè®­ç»ƒ
    # å‰©ä½™ 100 ä¸ªç”¨äºéªŒè¯
    
    # Stage 2 å‚æ•°
    n_neurons_range: Tuple[int, int] = (2, 4)
    max_overlap_ratio: float = 0.05
    min_centroid_distance: int = 40
    neuron_scale_range: Tuple[float, float] = (0.5, 0.75)
    max_placement_attempts: int = 100
    
    # æ ·æœ¬æ•°é‡
    train_samples: int = 800
    eval_samples: int = 160
    
    # æ•°æ®å¢å¼º
    enable_augmentation: bool = True
    flip_prob: float = 0.5          # ç¿»è½¬æ¦‚ç‡
    rotate90_prob: float = 0.5      # 90Â° æ—‹è½¬æ¦‚ç‡
    
    # å…¶ä»–
    random_seed: int = 42
    save_visualization: bool = True  # æ˜¯å¦ä¿å­˜å¯è§†åŒ–


# =========================
# zarr å…¼å®¹å·¥å…·
# =========================
def _zarr_version_major() -> int:
    try:
        return int(zarr.__version__.split('.')[0])
    except Exception:
        return 2


def _create_array_compat(group, name, data, chunks, compressor_instance):
    v = _zarr_version_major()
    if v >= 3:
        compressors = [compressor_instance] if compressor_instance is not None else None
        return group.create_array(name=name, data=data, chunks=chunks, compressors=compressors)
    else:
        return group.create_dataset(name, data=data, chunks=chunks, compressor=compressor_instance)


# =========================
# æ•°æ®å¢å¼º
# =========================
class Augmentor:
    """æ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self, flip_prob: float = 0.5, rotate90_prob: float = 0.5):
        self.flip_prob = flip_prob
        self.rotate90_prob = rotate90_prob
    
    def augment(self, raw: np.ndarray, gt: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Dict]:
        """
        å¯¹ raw (C, Z, Y, X) å’Œ gt (N, Z, Y, X) è¿›è¡Œç›¸åŒçš„å¢å¼º
        è¿”å›å¢å¼ºåçš„æ•°æ®å’Œå¢å¼ºè®°å½•
        """
        aug_info = {'flips': [], 'rotations': 0}
        
        # éšæœºç¿»è½¬ï¼ˆæ²¿ Z, Y, X è½´ï¼‰
        for axis, axis_name in [(1, 'Z'), (2, 'Y'), (3, 'X')]:
            if random.random() < self.flip_prob:
                raw = np.flip(raw, axis=axis)
                gt = np.flip(gt, axis=axis)
                aug_info['flips'].append(axis_name)
        
        # éšæœº 90Â° æ—‹è½¬ï¼ˆåœ¨ Y-X å¹³é¢ï¼‰
        if random.random() < self.rotate90_prob:
            k = random.randint(1, 3)  # æ—‹è½¬ 90Â°, 180Â°, æˆ– 270Â°
            raw = np.rot90(raw, k=k, axes=(2, 3))
            gt = np.rot90(gt, k=k, axes=(2, 3))
            aug_info['rotations'] = k * 90
        
        # ç¡®ä¿å†…å­˜è¿ç»­
        raw = np.ascontiguousarray(raw)
        gt = np.ascontiguousarray(gt)
        
        return raw, gt, aug_info


# =========================
# Stage 2 ç”Ÿæˆå™¨
# =========================
class Stage2Generator:
    """Stage 2 æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Stage2Config):
        self.config = config
        self.compressor = numcodecs.Blosc(cname='zstd', clevel=3, shuffle=numcodecs.Blosc.BITSHUFFLE)
        
        # æ•°æ®å¢å¼ºå™¨
        self.augmentor = Augmentor(
            flip_prob=config.flip_prob if config.enable_augmentation else 0,
            rotate90_prob=config.rotate90_prob if config.enable_augmentation else 0
        )
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.train_dir = os.path.join(config.output_dir, "train")
        self.eval_dir = os.path.join(config.output_dir, "eval")
        self.vis_dir = os.path.join(config.output_dir, "visualizations")
        
        os.makedirs(self.train_dir, exist_ok=True)
        os.makedirs(self.eval_dir, exist_ok=True)
        if config.save_visualization:
            os.makedirs(self.vis_dir, exist_ok=True)
        
        # åŠ è½½ç¥ç»å…ƒæ± 
        self._load_neuron_pools()
        
        # ä¿å­˜é…ç½®
        self._save_config()
    
    def _load_neuron_pools(self):
        """åŠ è½½å¹¶åˆ’åˆ†ç¥ç»å…ƒæ± """
        all_files = sorted([f for f in os.listdir(self.config.input_dir) if f.endswith('.zarr')])
        
        # åªä½¿ç”¨å‰ 500 ä¸ª
        available_files = all_files[:self.config.total_neurons_to_use]
        
        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯
        self.train_pool = available_files[:self.config.train_pool_size]
        self.eval_pool = available_files[self.config.train_pool_size:self.config.total_neurons_to_use]
        self.test_pool = all_files[self.config.total_neurons_to_use:]  # ç•™ä½œæµ‹è¯•
        
        print(f"ğŸ“¦ ç¥ç»å…ƒæ± åˆ’åˆ†:")
        print(f"   - æ€»æ•°: {len(all_files)}")
        print(f"   - è®­ç»ƒæ± : {len(self.train_pool)} (ç´¢å¼• 0-{self.config.train_pool_size - 1})")
        print(f"   - éªŒè¯æ± : {len(self.eval_pool)} (ç´¢å¼• {self.config.train_pool_size}-{self.config.total_neurons_to_use - 1})")
        print(f"   - æµ‹è¯•æ± ï¼ˆä¿ç•™ï¼‰: {len(self.test_pool)} (ç´¢å¼• {self.config.total_neurons_to_use}+)")
    
    def _save_config(self):
        """ä¿å­˜é…ç½®åˆ° JSON"""
        config_path = os.path.join(self.config.output_dir, "generation_config.json")
        config_dict = asdict(self.config)
        config_dict['generation_time'] = datetime.now().isoformat()
        config_dict['train_pool_files'] = self.train_pool
        config_dict['eval_pool_files'] = self.eval_pool
        
        with open(config_path, 'w') as f:
            json.dump(config_dict, f, indent=2)
        print(f"ğŸ“ é…ç½®å·²ä¿å­˜: {config_path}")
    
    # ---------- ç¥ç»å…ƒåŠ è½½ä¸å¤„ç† ----------
    def load_neuron(self, zarr_path: str) -> Optional[Dict]:
        try:
            zroot = zarr.open(zarr_path, mode='r')
            raw = zroot['volumes/raw'][:]
            gt = zroot['volumes/gt_instances'][:]
            if gt.ndim == 4 and gt.shape[0] == 1:
                gt = gt[0]
            if raw.ndim != 4 or gt.ndim != 3:
                return None
            if raw.dtype != np.uint8:
                raw = raw.astype(np.uint8)
            if gt.dtype != np.uint8:
                gt = (gt > 0).astype(np.uint8)
            return {'raw': raw, 'gt': gt}
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {zarr_path}, é”™è¯¯: {e}")
            return None
    
    def crop_to_bbox(self, raw: np.ndarray, gt: np.ndarray) -> Optional[Dict]:
        coords = np.argwhere(gt > 0)
        if coords.shape[0] == 0:
            return None
        z_min, y_min, x_min = coords.min(axis=0)
        z_max, y_max, x_max = coords.max(axis=0)
        cropped_gt = gt[z_min:z_max+1, y_min:y_max+1, x_min:x_max+1]
        cropped_raw = raw[:, z_min:z_max+1, y_min:y_max+1, x_min:x_max+1]
        return {'raw': cropped_raw, 'gt': cropped_gt}
    
    def scale_neuron(self, raw: np.ndarray, gt: np.ndarray, 
                     target_scale: float) -> Tuple[np.ndarray, np.ndarray, float]:
        tz, ty, tx = self.config.target_size
        nz, ny, nx = gt.shape
        
        max_scale = min(tz / nz, ty / ny, tx / nx)
        final_scale = min(target_scale, max_scale, 1.0)
        
        if final_scale >= 0.99:
            return raw, gt, 1.0
        
        gt_scaled = zoom(gt, final_scale, order=0)
        raw_scaled = np.stack([zoom(raw[c], final_scale, order=1) 
                               for c in range(raw.shape[0])]).astype(np.uint8)
        return raw_scaled, gt_scaled.astype(np.uint8), final_scale
    
    # ---------- æ”¾ç½®ç­–ç•¥ ----------
    def _get_grid_divisions(self, n_neurons: int) -> Tuple[int, int, int]:
        if n_neurons <= 2:
            return (1, 1, 2)
        elif n_neurons <= 4:
            return (1, 2, 2)
        else:
            return (2, 2, 2)
    
    def _grid_start(self, nz: int, ny: int, nx: int, 
                    grid_index: int, n_neurons: int) -> Tuple[int, int, int]:
        tz, ty, tx = self.config.target_size
        dz, dy, dx = self._get_grid_divisions(n_neurons)
        
        iz = (grid_index // (dy * dx)) % dz
        iy = (grid_index // dx) % dy
        ix = grid_index % dx
        
        cell_z, cell_y, cell_x = tz // dz, ty // dy, tx // dx
        
        z_start, y_start, x_start = iz * cell_z, iy * cell_y, ix * cell_x
        z_end = min((iz + 1) * cell_z, tz) - nz
        y_end = min((iy + 1) * cell_y, ty) - ny
        x_end = min((ix + 1) * cell_x, tx) - nx
        
        sz = random.randint(z_start, max(z_start, z_end))
        sy = random.randint(y_start, max(y_start, y_end))
        sx = random.randint(x_start, max(x_start, x_end))
        
        return (sz, sy, sx)
    
    def _random_start(self, nz: int, ny: int, nx: int) -> Tuple[int, int, int]:
        tz, ty, tx = self.config.target_size
        return (
            random.randint(0, max(0, tz - nz)),
            random.randint(0, max(0, ty - ny)),
            random.randint(0, max(0, tx - nx))
        )
    
    # ---------- ç¢°æ’æ£€æµ‹ ----------
    def _compute_overlap(self, mask: np.ndarray, gt: np.ndarray, 
                         start: Tuple[int, int, int]) -> float:
        tz, ty, tx = self.config.target_size
        nz, ny, nx = gt.shape
        sz, sy, sx = start
        
        if sz < 0 or sy < 0 or sx < 0:
            return 1.0
        if sz + nz > tz or sy + ny > ty or sx + nx > tx:
            return 1.0
        
        existing = mask[sz:sz+nz, sy:sy+ny, sx:sx+nx] > 0
        new_region = gt > 0
        new_count = np.sum(new_region)
        
        if new_count == 0:
            return 0.0
        return np.sum(new_region & existing) / new_count
    
    def _check_distance(self, centroids: List[Tuple], gt: np.ndarray, 
                        start: Tuple[int, int, int]) -> bool:
        if len(centroids) == 0:
            return True
        
        local = center_of_mass(gt)
        global_c = (start[0] + local[0], start[1] + local[1], start[2] + local[2])
        
        for c in centroids:
            dist = np.sqrt(sum((a - b) ** 2 for a, b in zip(global_c, c)))
            if dist < self.config.min_centroid_distance:
                return False
        return True
    
    # ---------- ç²˜è´´ ----------
    def _paste(self, combined_raw: np.ndarray, combined_gt: np.ndarray,
               combined_mask: np.ndarray, src_raw: np.ndarray,
               src_gt: np.ndarray, start: Tuple[int, int, int],
               neuron_idx: int) -> bool:
        tz, ty, tx = self.config.target_size
        nz, ny, nx = src_gt.shape
        sz, sy, sx = start
        
        if sz < 0 or sy < 0 or sx < 0:
            return False
        if sz + nz > tz or sy + ny > ty or sx + nx > tx:
            return False
        
        mask = src_gt > 0
        n_ch = min(3, src_raw.shape[0])
        
        for c in range(n_ch):
            target = combined_raw[c, sz:sz+nz, sy:sy+ny, sx:sx+nx]
            np.maximum(target, src_raw[c], out=target, where=mask)
        
        for c in range(n_ch, 3):
            target = combined_raw[c, sz:sz+nz, sy:sy+ny, sx:sx+nx]
            np.maximum(target, src_raw[0], out=target, where=mask)
        
        combined_gt[neuron_idx, sz:sz+nz, sy:sy+ny, sx:sx+nx][mask] = 1
        combined_mask[sz:sz+nz, sy:sy+ny, sx:sx+nx][mask] = 1
        return True
    
    # ---------- ç”Ÿæˆå•ä¸ªæ ·æœ¬ ----------
    def generate_sample(self, n_neurons: int, output_path: str,
                        neuron_pool: List[str], seed: int) -> Dict:
        random.seed(seed)
        np.random.seed(seed)
        
        candidates = random.sample(neuron_pool, min(n_neurons * 4, len(neuron_pool)))
        
        tz, ty, tx = self.config.target_size
        combined_raw = np.zeros((3, tz, ty, tx), dtype=np.uint8)
        combined_gt = np.zeros((n_neurons, tz, ty, tx), dtype=np.uint8)
        combined_mask = np.zeros((tz, ty, tx), dtype=np.uint8)
        
        centroids = []
        info = []
        placed = 0
        
        for fname in candidates:
            if placed >= n_neurons:
                break
            
            # åŠ è½½
            data = self.load_neuron(os.path.join(self.config.input_dir, fname))
            if data is None:
                continue
            
            cropped = self.crop_to_bbox(data['raw'], data['gt'])
            if cropped is None:
                continue
            
            # ç¼©æ”¾
            scale = random.uniform(*self.config.neuron_scale_range)
            raw_s, gt_s, actual_scale = self.scale_neuron(cropped['raw'], cropped['gt'], scale)
            
            nz, ny, nx = gt_s.shape
            if nz > tz or ny > ty or nx > tx:
                continue
            
            # æ”¾ç½®
            success = False
            for attempt in range(self.config.max_placement_attempts):
                if attempt < self.config.max_placement_attempts // 2:
                    start = self._grid_start(nz, ny, nx, placed, n_neurons)
                else:
                    start = self._random_start(nz, ny, nx)
                
                if self._compute_overlap(combined_mask, gt_s, start) > self.config.max_overlap_ratio:
                    continue
                if not self._check_distance(centroids, gt_s, start):
                    continue
                
                success = True
                break
            
            if not success:
                continue
            
            # ç²˜è´´
            if not self._paste(combined_raw, combined_gt, combined_mask, raw_s, gt_s, start, placed):
                continue
            
            # è®°å½•
            local = center_of_mass(gt_s)
            global_c = (start[0] + local[0], start[1] + local[1], start[2] + local[2])
            centroids.append(global_c)
            
            info.append({
                'id': placed + 1,
                'file': fname,
                'scale': float(actual_scale),
                'start': start,
                'centroid': tuple(float(x) for x in global_c)
            })
            placed += 1
        
        if placed == 0:
            return {'status': 'error', 'error': 'æ— æ³•æ”¾ç½®ä»»ä½•ç¥ç»å…ƒ'}
        
        if placed < n_neurons:
            combined_gt = combined_gt[:placed]
        
        # æ•°æ®å¢å¼º
        combined_raw, combined_gt, aug_info = self.augmentor.augment(combined_raw, combined_gt)
        
        # ä¿å­˜
        return self._save_sample(combined_raw, combined_gt, output_path, info, aug_info, seed)
    
    # ---------- ä¿å­˜ ----------
    def _save_sample(self, raw: np.ndarray, gt: np.ndarray, output_path: str,
                     neurons_info: List[Dict], aug_info: Dict, seed: int) -> Dict:
        try:
            os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
            
            zroot = zarr.open(output_path, mode='w')
            g = zroot.create_group('volumes')
            _create_array_compat(g, 'raw', raw, (1, 64, 128, 128), self.compressor)
            _create_array_compat(g, 'gt_instances', gt, (1, 64, 128, 128), self.compressor)
            
            zroot.attrs['info'] = {
                'stage': 2,
                'n_neurons': len(neurons_info),
                'target_size': self.config.target_size,
                'max_overlap_ratio': self.config.max_overlap_ratio,
                'min_centroid_distance': self.config.min_centroid_distance,
                'augmentation': aug_info,
                'seed': seed,
                'neurons': neurons_info
            }
            
            return {'status': 'success', 'path': output_path, 'n_neurons': len(neurons_info)}
        except Exception as e:
            traceback.print_exc()
            return {'status': 'error', 'error': str(e)}
    
    # ---------- å¯è§†åŒ– ----------
    def save_visualization(self, zarr_path: str, output_prefix: str):
        """ç”Ÿæˆæœ€å¤§æŠ•å½±å¯è§†åŒ–"""
        try:
            zroot = zarr.open(zarr_path, mode='r')
            raw = zroot['volumes/raw'][:]
            gt = zroot['volumes/gt_instances'][:]
            
            # æœ€å¤§æŠ•å½±
            raw_mip = np.max(raw, axis=1)  # (3, Y, X)
            
            # GT åˆå¹¶å¹¶ç€è‰²
            n_neurons = gt.shape[0]
            colors = [
                [255, 0, 0], [0, 255, 0], [0, 0, 255],
                [255, 255, 0], [255, 0, 255], [0, 255, 255],
                [128, 0, 0], [0, 128, 0], [0, 0, 128]
            ]
            
            gt_colored = np.zeros((3, gt.shape[2], gt.shape[3]), dtype=np.uint8)
            for i in range(n_neurons):
                gt_mip = np.max(gt[i], axis=0)
                color = colors[i % len(colors)]
                for c in range(3):
                    gt_colored[c][gt_mip > 0] = color[c]
            
            # ä¿å­˜ä¸º .npyï¼ˆå¯ä»¥ç”¨å…¶ä»–å·¥å…·è½¬æ¢ä¸ºå›¾ç‰‡ï¼‰
            np.save(f"{output_prefix}_raw_mip.npy", raw_mip)
            np.save(f"{output_prefix}_gt_mip.npy", gt_colored)
            
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
    
    # ---------- ä¸»æµç¨‹ ----------
    def generate_all(self):
        """ç”Ÿæˆæ‰€æœ‰æ•°æ®"""
        random.seed(self.config.random_seed)
        
        print("\n" + "=" * 60)
        print("ğŸš€ Stage 2 æ•°æ®ç”Ÿæˆ")
        print("=" * 60)
        print(f"ğŸ“Š é…ç½®:")
        print(f"   - ç¥ç»å…ƒæ•°é‡: {self.config.n_neurons_range}")
        print(f"   - æœ€å¤§é‡å ç‡: {self.config.max_overlap_ratio * 100}%")
        print(f"   - è´¨å¿ƒè·ç¦»: {self.config.min_centroid_distance}")
        print(f"   - ç¼©æ”¾èŒƒå›´: {self.config.neuron_scale_range}")
        print(f"   - æ•°æ®å¢å¼º: {'å¼€å¯' if self.config.enable_augmentation else 'å…³é—­'}")
        print()
        
        stats = {'train': [], 'eval': []}
        
        # è®­ç»ƒé›†
        print("ğŸ“ ç”Ÿæˆè®­ç»ƒé›†...")
        for i in range(self.config.train_samples):
            n = random.randint(*self.config.n_neurons_range)
            path = os.path.join(self.train_dir, f"stage2_{i:04d}.zarr")
            result = self.generate_sample(n, path, self.train_pool, seed=2000 + i)
            
            if result['status'] == 'success':
                stats['train'].append(result['n_neurons'])
                if (i + 1) % 100 == 0:
                    print(f"   âœ… {i + 1}/{self.config.train_samples}")
            else:
                print(f"   âŒ [{i}] {result.get('error')}")
        
        # éªŒè¯é›†
        print("\nğŸ“ ç”ŸæˆéªŒè¯é›†...")
        for i in range(self.config.eval_samples):
            n = random.randint(*self.config.n_neurons_range)
            path = os.path.join(self.eval_dir, f"stage2_{i:04d}.zarr")
            result = self.generate_sample(n, path, self.eval_pool, seed=3000 + i)
            
            if result['status'] == 'success':
                stats['eval'].append(result['n_neurons'])
                if (i + 1) % 50 == 0:
                    print(f"   âœ… {i + 1}/{self.config.eval_samples}")
            else:
                print(f"   âŒ [{i}] {result.get('error')}")
        
        # å¯è§†åŒ–ï¼ˆå‰å‡ ä¸ªæ ·æœ¬ï¼‰
        if self.config.save_visualization:
            print("\nğŸ“¸ ç”Ÿæˆå¯è§†åŒ–...")
            for i in range(min(5, len(stats['train']))):
                zarr_path = os.path.join(self.train_dir, f"stage2_{i:04d}.zarr")
                vis_prefix = os.path.join(self.vis_dir, f"train_{i:04d}")
                self.save_visualization(zarr_path, vis_prefix)
            print(f"   ä¿å­˜è‡³: {self.vis_dir}")
        
        # ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡")
        print("=" * 60)
        for split in ['train', 'eval']:
            counts = stats[split]
            if counts:
                print(f"   [{split}]")
                print(f"   - æˆåŠŸ: {len(counts)} æ ·æœ¬")
                print(f"   - ç¥ç»å…ƒæ•°: min={min(counts)}, max={max(counts)}, avg={np.mean(counts):.2f}")
        
        print(f"\nğŸ“‚ è¾“å‡ºç›®å½•: {self.config.output_dir}")
        print("âœ¨ å®Œæˆ!")


# =============================
# ä¸»ç¨‹åº
# =============================
if __name__ == "__main__":
    config = Stage2Config(
        # === è·¯å¾„ ===
        input_dir="/data/wangfeiran/code/brainbow/datasets/fisbe/single_neurons",
        output_dir="/data/wangfeiran/code/brainbow/datasets/fisbe/curriculum_data/stage2",
        
        # === æ•°æ®åˆ’åˆ†ï¼ˆåªç”¨å‰500ä¸ªï¼‰===
        total_neurons_to_use=500,   # åªä½¿ç”¨å‰ 500 ä¸ª
        train_pool_size=400,        # 400 è®­ç»ƒ + 100 éªŒè¯
        
        # === ä½“ç§¯å¤§å° ===
        target_size=(200, 250, 250),
        
        # === Stage 2 å‚æ•° ===
        n_neurons_range=(2, 4),
        max_overlap_ratio=0.05,
        min_centroid_distance=40,
        neuron_scale_range=(0.5, 0.75),
        max_placement_attempts=100,
        
        # === æ ·æœ¬æ•°é‡ ===
        train_samples=800,
        eval_samples=160,
        
        # === æ•°æ®å¢å¼º ===
        enable_augmentation=True,
        flip_prob=0.5,
        rotate90_prob=0.5,
        
        # === å…¶ä»– ===
        random_seed=42,
        save_visualization=True,
    )
    
    generator = Stage2Generator(config)
    generator.generate_all()